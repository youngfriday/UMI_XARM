name: square_image

shape_meta: &shape_meta
  # acceptable types: rgb, low_dim
  obs:
    agentview_image:
      shape: [3, 84, 84]
      horizon: 2
      type: rgb
    robot0_eye_in_hand_image:
      shape: [3, 84, 84]
      horizon: 2
      type: rgb
    robot0_eef_pos:
      shape: [3]
      horizon: 2
      type: low_dim
    robot0_eef_quat:
      raw_shape: [4]
      shape: [6]
      horizon: 2
      type: low_dim
      rotation_rep: rotation_6d
    robot0_gripper_qpos:
      shape: [2]
      horizon: 2
      type: low_dim
  action: 
    shape: [10]
    rotation_rep: rotation_6d

task_name: &task_name square
dataset_type: &dataset_type ph
dataset_path: &dataset_path data/robomimic/datasets/${task.task_name}/${task.dataset_type}/image_abs.hdf5
pose_repr: &pose_repr
  obs_pose_repr: rel # abs or rel
  action_pose_repr: rel

env_runner:
  _target_: diffusion_policy.env_runner.robomimic_runner.RobomimicRunner
  dataset_path: *dataset_path
  pose_repr: *pose_repr
  shape_meta: *shape_meta
  n_train: 6
  n_train_vis: 2
  train_start_idx: 0
  n_test: 50
  n_test_vis: 4
  test_start_seed: 100000
  # use python's eval function as resolver, single-quoted string as argument
  max_steps: ${eval:'500 if "${task.dataset_type}" == "mh" else 400'}
  n_obs_steps: ${n_obs_steps}
  n_action_steps: ${n_action_steps}
  render_obs_key: 'agentview_image'
  fps: 10
  crf: 22
  tqdm_interval_sec: 1.0
  n_envs: 28
# evaluation at this config requires a 16 core 64GB instance.

dataset:
  _target_: diffusion_policy.dataset.robomimic_replay_dataset.RobomimicReplayDataset
  pose_repr: *pose_repr
  shape_meta: *shape_meta
  dataset_path: *dataset_path
  horizon: ${horizon}
  pad_before: ${eval:'${n_obs_steps}-1+${n_latency_steps}'}
  pad_after: ${eval:'${horizon}-${n_obs_steps}'}
  n_obs_steps: ${dataset_obs_steps}
  use_cache: True
  seed: 42
  val_ratio: 0.02